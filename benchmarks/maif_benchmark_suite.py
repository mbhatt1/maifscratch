#!/usr/bin/env python3
"""
MAIF Benchmark Suite - Validates claims from the research paper

This benchmark suite tests the key performance and capability claims made in the
"An Artifact-Centric AI Agent Design and the Multimodal Artifact File Format (MAIF)
for Enhanced Trustworthiness" paper.

Key Claims to Validate:
1. Performance Claims:
   - 2.5-5× compression ratios for text
   - Sub-50ms semantic search on commodity hardware
   - 500+ MB/s streaming throughput
   - 95%+ automated repair success rates
   - <15% cryptographic overhead

2. Security Claims:
   - 100% tamper detection within 1ms verification
   - Immutable provenance chains
   - Block-level integrity verification

3. Functionality Claims:
   - Multimodal data integration
   - Semantic embedding and search
   - Cross-modal attention mechanisms
   - Privacy-by-design features
"""

import os
import sys
import time
import json
import random
import hashlib
import tempfile
import statistics
from typing import Dict, List, Tuple, Any
from pathlib import Path
import numpy as np

# Add the parent directory to the path to import maif
sys.path.insert(0, str(Path(__file__).parent.parent))

from maif.core import MAIFEncoder, MAIFDecoder, MAIFParser
from maif.semantic import SemanticEmbedder
from maif.security import MAIFSigner, MAIFVerifier
from maif.validation import MAIFValidator, MAIFRepairTool
from maif.streaming import MAIFStreamReader, StreamingConfig
from maif.compression import MAIFCompressor, CompressionAlgorithm
from maif.privacy import PrivacyEngine, PrivacyPolicy, PrivacyLevel, EncryptionMode

class BenchmarkResult:
    """Container for benchmark results."""
    
    def __init__(self, name: str):
        self.name = name
        self.metrics: Dict[str, Any] = {}
        self.success = True
        self.error_message = ""
        self.start_time = 0
        self.end_time = 0
    
    def add_metric(self, key: str, value: Any):
        """Add a metric to the results."""
        self.metrics[key] = value
    
    def set_error(self, message: str):
        """Mark benchmark as failed with error message."""
        self.success = False
        self.error_message = message
    
    def duration(self) -> float:
        """Get benchmark duration in seconds."""
        return self.end_time - self.start_time

class MAIFBenchmarkSuite:
    """Comprehensive benchmark suite for MAIF implementation."""
    
    def __init__(self, output_dir: str = "benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results: List[BenchmarkResult] = []
        
        # Test data sizes
        self.text_sizes = [1024, 10240, 102400, 1048576]  # 1KB to 1MB
        self.embedding_counts = [100, 1000, 10000, 100000]
        self.file_counts = [10, 100, 1000]
        
        print(f"MAIF Benchmark Suite initialized")
        print(f"Results will be saved to: {self.output_dir}")
    
    def run_all_benchmarks(self) -> Dict[str, Any]:
        """Run all benchmark tests."""
        print("\n" + "="*80)
        print("MAIF BENCHMARK SUITE - VALIDATING PAPER CLAIMS")
        print("="*80)
        
        # Core functionality benchmarks
        self._benchmark_compression_ratios()
        self._benchmark_semantic_search_performance()
        # Skip large-scale test in quick mode for faster execution
        if not hasattr(self, 'quick_mode') or not self.quick_mode:
            self._benchmark_large_scale_semantic_search()
        self._benchmark_streaming_throughput()
        self._benchmark_cryptographic_overhead()
        self._benchmark_tamper_detection()
        self._benchmark_integrity_verification()
        
        # Advanced feature benchmarks
        self._benchmark_multimodal_integration()
        self._benchmark_provenance_chains()
        self._benchmark_privacy_features()
        self._benchmark_repair_capabilities()
        self._benchmark_scalability()
        
        # Video functionality benchmarks
        self._benchmark_video_storage_performance()
        self._benchmark_video_metadata_extraction()
        self._benchmark_video_querying_performance()
        
        # Concurrent operation benchmarks
        self._benchmark_concurrent_read_write()
        self._benchmark_read_during_write()
        self._benchmark_write_during_read()
        self._benchmark_lock_contention()
        self._benchmark_concurrent_block_access()
        
        # Novel algorithms benchmarks
        self._benchmark_novel_algorithms()
        
        # Generate comprehensive report
        return self._generate_report()
    
    def _benchmark_compression_ratios(self):
        """Benchmark compression ratios - Paper claims 2.5-5× for text."""
        result = BenchmarkResult("Compression Ratios")
        result.start_time = time.time()
        
        try:
            compressor = MAIFCompressor()
            compression_results = {}
            
            # Test different text types and sizes
            test_texts = {
                "lorem_ipsum": self._generate_lorem_ipsum(10000),
                "json_data": json.dumps(self._generate_test_json(1000), indent=2),
                "code_sample": self._generate_code_sample(5000),
                "repeated_text": "Hello MAIF! " * 1000,
                "random_text": self._generate_random_text(10000)
            }
            
            for text_type, text in test_texts.items():
                original_size = len(text.encode('utf-8'))
                
                # Test different compression algorithms
                for algorithm in [CompressionAlgorithm.ZLIB, CompressionAlgorithm.BROTLI, CompressionAlgorithm.LZMA]:
                    compressed = compressor.compress(text.encode('utf-8'), algorithm)
                    compressed_size = len(compressed)
                    ratio = original_size / compressed_size
                    
                    compression_results[f"{text_type}_{algorithm.value}"] = {
                        "original_size": original_size,
                        "compressed_size": compressed_size,
                        "ratio": ratio,
                        "reduction_percent": (1 - compressed_size/original_size) * 100
                    }
            
            # Calculate average ratios
            ratios = [r["ratio"] for r in compression_results.values()]
            avg_ratio = statistics.mean(ratios)
            max_ratio = max(ratios)
            min_ratio = min(ratios)
            
            result.add_metric("compression_results", compression_results)
            result.add_metric("average_ratio", avg_ratio)
            result.add_metric("max_ratio", max_ratio)
            result.add_metric("min_ratio", min_ratio)
            result.add_metric("claim_validation", {
                "paper_claim": "2.5-5× compression ratios",
                "achieved_avg": avg_ratio,
                "achieved_max": max_ratio,
                "meets_claim": avg_ratio >= 2.5  # Meeting or exceeding is success
            })
            
        except Exception as e:
            result.set_error(f"Compression benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Compression Ratios: Avg {result.metrics.get('average_ratio', 0):.2f}×")
    
    def _benchmark_semantic_search_performance(self):
        """Benchmark semantic search - Paper claims sub-50ms on commodity hardware."""
        result = BenchmarkResult("Semantic Search Performance")
        result.start_time = time.time()
        
        try:
            # Use optimized embedder for better performance
            try:
                from maif.semantic_optimized import OptimizedSemanticEmbedder
                embedder = OptimizedSemanticEmbedder(use_gpu=True)
                print("  Using optimized semantic embedder with GPU acceleration")
            except ImportError:
                from maif.semantic import SemanticEmbedder
                embedder = SemanticEmbedder()
                print("  Using standard semantic embedder")
            search_times = []
            
            # Create test corpus
            test_texts = [
                f"Document {i}: This is test content about topic {i%10}"
                for i in range(1000)
            ]
            
            # Generate embeddings
            print("  Generating embeddings for search benchmark...")
            embeddings = embedder.embed_texts(test_texts)
            
            # Perform search tests
            query_texts = [
                "topic 5",
                "test content",
                "document information",
                "specific topic",
                "content about"
            ]
            
            for query in query_texts:
                query_embedding = embedder.embed_text(query)
                
                # Time the search operation
                search_start = time.time()
                similarities = []
                for emb in embeddings:
                    sim = embedder.compute_similarity(query_embedding, emb)
                    similarities.append(sim)
                
                # Find top results
                top_indices = np.argsort(similarities)[-10:]
                search_end = time.time()
                
                search_time_ms = (search_end - search_start) * 1000
                search_times.append(search_time_ms)
            
            avg_search_time = statistics.mean(search_times)
            max_search_time = max(search_times)
            min_search_time = min(search_times)
            
            result.add_metric("search_times_ms", search_times)
            result.add_metric("average_search_time_ms", avg_search_time)
            result.add_metric("max_search_time_ms", max_search_time)
            result.add_metric("min_search_time_ms", min_search_time)
            result.add_metric("corpus_size", len(test_texts))
            result.add_metric("claim_validation", {
                "paper_claim": "Sub-50ms semantic search",
                "achieved_avg": avg_search_time,
                "achieved_max": max_search_time,
                "meets_claim": avg_search_time < 50.0
            })
            
        except Exception as e:
            result.set_error(f"Semantic search benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Semantic Search: Avg {result.metrics.get('average_search_time_ms', 0):.1f}ms")
    def _benchmark_large_scale_semantic_search(self):
        """Benchmark extremely large-scale semantic search - Stress test with 100K+ documents."""
        result = BenchmarkResult("Large-Scale Semantic Search")
        result.start_time = time.time()
        
        try:
            # Use optimized embedder for large-scale testing
            try:
                from maif.semantic_optimized import OptimizedSemanticEmbedder
                embedder = OptimizedSemanticEmbedder(use_gpu=True)
                print("  Using optimized semantic embedder with GPU acceleration")
            except ImportError:
                from maif.semantic import SemanticEmbedder
                embedder = SemanticEmbedder()
                print("  Using standard semantic embedder (performance will be limited)")
            search_times = []
            
            # Create extremely large test corpus (focus on search performance)
            corpus_sizes = [100000]  # 100k embeddings for stress testing
            
            for corpus_size in corpus_sizes:
                print(f"  Testing semantic search with {corpus_size:,} documents...")
                
                # Generate diverse test corpus
                test_texts = []
                topics = ["technology", "science", "medicine", "finance", "education", 
                         "environment", "politics", "sports", "entertainment", "travel"]
                
                for i in range(corpus_size):
                    topic = topics[i % len(topics)]
                    # Create more realistic document content
                    test_texts.append(
                        f"Document {i}: Advanced research in {topic} reveals new insights. "
                        f"This comprehensive study examines {topic} methodologies and their "
                        f"applications in modern contexts. The findings suggest significant "
                        f"implications for future {topic} development and implementation strategies."
                    )
                
                # Generate embeddings with optimized batch processing
                print(f"    Generating {corpus_size:,} embeddings...")
                embedding_start = time.time()
                
                if hasattr(embedder, 'embed_texts_batch'):
                    # Use optimized batch processing
                    embeddings = embedder.embed_texts_batch(test_texts, batch_size=64)
                else:
                    # Fallback to standard method
                    embeddings = embedder.embed_texts(test_texts)
                
                embedding_time = time.time() - embedding_start
                print(f"    Embedding generation: {embedding_time:.2f}s ({corpus_size/embedding_time:.0f} docs/sec)")
                
                # Build search index for fast retrieval
                if hasattr(embedder, 'build_search_index'):
                    print(f"    Building search index...")
                    index_start = time.time()
                    embedder.build_search_index(embeddings)
                    index_time = time.time() - index_start
                    print(f"    Index building: {index_time:.2f}s")
                
                # Complex search queries
                complex_queries = [
                    "advanced research methodologies in technology",
                    "comprehensive study of scientific applications",
                    "modern medical research findings",
                    "financial development strategies",
                    "educational implementation approaches",
                    "environmental policy implications",
                    "political analysis frameworks",
                    "sports performance optimization",
                    "entertainment industry trends",
                    "sustainable travel solutions"
                ]
                
                corpus_search_times = []
                
                for query in complex_queries:
                    # Generate query embedding
                    if hasattr(embedder, 'embed_text_single'):
                        query_embedding = embedder.embed_text_single(query)
                    else:
                        query_embedding = embedder.embed_text(query)
                    
                    # Time the search operation
                    search_start = time.time()
                    
                    if hasattr(embedder, 'search_similar'):
                        # Use optimized FAISS search
                        results = embedder.search_similar(query_embedding, top_k=20)
                        top_indices = [idx for idx, sim in results]
                    else:
                        # Fallback to brute-force search
                        similarities = []
                        for emb in embeddings:
                            sim = embedder.compute_similarity(query_embedding, emb)
                            similarities.append(sim)
                        top_indices = np.argsort(similarities)[-20:]
                    
                    search_end = time.time()
                    search_time_ms = (search_end - search_start) * 1000
                    corpus_search_times.append(search_time_ms)
                
                avg_search_time = statistics.mean(corpus_search_times)
                max_search_time = max(corpus_search_times)
                min_search_time = min(corpus_search_times)
                
                print(f"    Search performance: Avg {avg_search_time:.1f}ms, Max {max_search_time:.1f}ms")
                
                # Store results for this corpus size
                result.add_metric(f"corpus_{corpus_size}_search_times_ms", corpus_search_times)
                result.add_metric(f"corpus_{corpus_size}_avg_search_ms", avg_search_time)
                result.add_metric(f"corpus_{corpus_size}_max_search_ms", max_search_time)
                result.add_metric(f"corpus_{corpus_size}_min_search_ms", min_search_time)
                result.add_metric(f"corpus_{corpus_size}_embedding_time_s", embedding_time)
                result.add_metric(f"corpus_{corpus_size}_docs_per_sec", corpus_size/embedding_time)
                
                search_times.extend(corpus_search_times)
            
            # Overall statistics
            overall_avg = statistics.mean(search_times)
            overall_max = max(search_times)
            overall_min = min(search_times)
            
            result.add_metric("overall_search_times_ms", search_times)
            result.add_metric("overall_average_search_time_ms", overall_avg)
            result.add_metric("overall_max_search_time_ms", overall_max)
            result.add_metric("overall_min_search_time_ms", overall_min)
            result.add_metric("total_documents_tested", sum(corpus_sizes))
            result.add_metric("total_queries_executed", len(search_times))
            
            # Performance scaling analysis
            scaling_efficiency = []
            base_time = result.metrics.get("corpus_10000_avg_search_ms", 0)
            for size in corpus_sizes:
                current_time = result.metrics.get(f"corpus_{size}_avg_search_ms", 0)
                if base_time > 0:
                    scaling_factor = current_time / base_time
                    expected_scaling = size / 10000  # Linear scaling expectation
                    efficiency = expected_scaling / scaling_factor if scaling_factor > 0 else 0
                    scaling_efficiency.append(efficiency)
            
            result.add_metric("scaling_efficiency", scaling_efficiency)
            result.add_metric("claim_validation", {
                "paper_claim": "Sub-50ms semantic search on large corpora",
                "achieved_avg": overall_avg,
                "achieved_max": overall_max,
                "largest_corpus": max(corpus_sizes),
                "meets_claim_avg": overall_avg < 50.0,
                "meets_claim_max": overall_max < 100.0  # More lenient for large corpus
            })
            
        except Exception as e:
            result.set_error(f"Large-scale semantic search benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Large-Scale Semantic Search: Avg {result.metrics.get('overall_average_search_time_ms', 0):.1f}ms across {result.metrics.get('total_documents_tested', 0):,} documents")

    def _benchmark_streaming_throughput(self):
        """Benchmark streaming throughput - Paper claims 500+ MB/s."""
        result = BenchmarkResult("Streaming Throughput")
        result.start_time = time.time()
        
        try:
            # Create large test file
            with tempfile.TemporaryDirectory() as tmpdir:
                encoder = MAIFEncoder()
                
                # Add multiple large blocks
                total_data_size = 0
                for i in range(10):
                    large_text = self._generate_random_text(1024 * 1024)  # 1MB each
                    encoder.add_text_block(large_text)
                    total_data_size += len(large_text.encode('utf-8'))
                
                maif_path = os.path.join(tmpdir, "large_test.maif")
                manifest_path = os.path.join(tmpdir, "large_test_manifest.json")
                encoder.build_maif(maif_path, manifest_path)
                
                # Test streaming read performance
                config = StreamingConfig(chunk_size=8192, max_workers=4)
                
                stream_start = time.time()
                bytes_read = 0
                
                with MAIFStreamReader(maif_path, config) as reader:
                    for block_id, data in reader.stream_blocks_parallel():
                        bytes_read += len(data)
                
                stream_end = time.time()
                duration = stream_end - stream_start
                throughput_mbps = (bytes_read / (1024 * 1024)) / duration
                
                result.add_metric("total_bytes_read", bytes_read)
                result.add_metric("duration_seconds", duration)
                result.add_metric("throughput_mbps", throughput_mbps)
                result.add_metric("claim_validation", {
                    "paper_claim": "500+ MB/s streaming",
                    "achieved": throughput_mbps,
                    "meets_claim": throughput_mbps >= 500.0
                })
                
        except Exception as e:
            result.set_error(f"Streaming benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Streaming Throughput: {result.metrics.get('throughput_mbps', 0):.1f} MB/s")
    
    def _benchmark_cryptographic_overhead(self):
        """Benchmark cryptographic overhead - Paper claims <15%."""
        result = BenchmarkResult("Cryptographic Overhead")
        result.start_time = time.time()
        
        try:
            test_data = self._generate_random_text(10000).encode('utf-8')  # Smaller test data for more accurate measurement
            
            # Pre-create encoders to isolate crypto operations
            encoder_no_crypto = MAIFEncoder(enable_privacy=False)
            encoder_crypto = MAIFEncoder(enable_privacy=True)
            
            # Set up crypto policy once
            from maif.privacy import PrivacyPolicy, PrivacyLevel, EncryptionMode
            crypto_policy = PrivacyPolicy(
                privacy_level=PrivacyLevel.INTERNAL,
                encryption_mode=EncryptionMode.AES_GCM,
                anonymization_required=False,
                audit_required=False
            )
            encoder_crypto.set_default_privacy_policy(crypto_policy)
            
            # Benchmark without crypto - measure only the block addition
            no_crypto_times = []
            for i in range(20):  # More iterations for better accuracy
                start = time.time()
                encoder_no_crypto.add_binary_block(test_data, f"test_data_{i}")
                end = time.time()
                no_crypto_times.append(end - start)
            
            # Benchmark with crypto - measure only the block addition with encryption
            crypto_times = []
            for i in range(20):  # More iterations for better accuracy
                start = time.time()
                encoder_crypto.add_binary_block(test_data, f"test_data_crypto_{i}")
                end = time.time()
                crypto_times.append(end - start)
            
            # Remove outliers for more accurate measurement
            no_crypto_times.sort()
            crypto_times.sort()
            
            # Use median of middle 50% to reduce noise
            trim_count = len(no_crypto_times) // 4
            no_crypto_trimmed = no_crypto_times[trim_count:-trim_count] if trim_count > 0 else no_crypto_times
            crypto_trimmed = crypto_times[trim_count:-trim_count] if trim_count > 0 else crypto_times
            
            avg_no_crypto = statistics.mean(no_crypto_trimmed)
            avg_crypto = statistics.mean(crypto_trimmed)
            overhead_percent = ((avg_crypto - avg_no_crypto) / avg_no_crypto) * 100
            
            result.add_metric("no_crypto_avg_time", avg_no_crypto)
            result.add_metric("crypto_avg_time", avg_crypto)
            result.add_metric("overhead_percent", overhead_percent)
            result.add_metric("no_crypto_times", no_crypto_times)
            result.add_metric("crypto_times", crypto_times)
            result.add_metric("claim_validation", {
                "paper_claim": "<15% cryptographic overhead",
                "achieved": overhead_percent,
                "meets_claim": overhead_percent < 15.0
            })
            
        except Exception as e:
            result.set_error(f"Cryptographic overhead benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Cryptographic Overhead: {result.metrics.get('overhead_percent', 0):.1f}%")
    
    def _benchmark_tamper_detection(self):
        """Benchmark tamper detection - Paper claims 100% detection within 1ms."""
        result = BenchmarkResult("Tamper Detection")
        result.start_time = time.time()
        
        try:
            detection_times = []
            detection_successes = 0
            total_tests = 100
            
            with tempfile.TemporaryDirectory() as tmpdir:
                for test_num in range(total_tests):
                    # Create test file
                    encoder = MAIFEncoder()
                    test_text = f"Test content {test_num}"
                    encoder.add_text_block(test_text)
                    
                    maif_path = os.path.join(tmpdir, f"test_{test_num}.maif")
                    manifest_path = os.path.join(tmpdir, f"test_{test_num}_manifest.json")
                    encoder.build_maif(maif_path, manifest_path)
                    
                    # Tamper with the file - corrupt within actual file bounds
                    file_size = os.path.getsize(maif_path)
                    if file_size > 32:  # Ensure we have data beyond header
                        with open(maif_path, 'r+b') as f:
                            # Corrupt somewhere in the data section (after 32-byte header)
                            corrupt_pos = random.randint(32, file_size - 1)
                            f.seek(corrupt_pos)
                            f.write(b'X')  # Corrupt one byte
                    else:
                        # File too small, skip this test
                        continue
                    
                    # Test detection
                    detection_start = time.time()
                    decoder = MAIFDecoder(maif_path, manifest_path)
                    is_valid = decoder.verify_integrity()
                    detection_end = time.time()
                    
                    detection_time_ms = (detection_end - detection_start) * 1000
                    detection_times.append(detection_time_ms)
                    
                    if not is_valid:  # Should detect tampering
                        detection_successes += 1
            
            avg_detection_time = statistics.mean(detection_times)
            max_detection_time = max(detection_times)
            detection_rate = (detection_successes / total_tests) * 100
            
            result.add_metric("detection_times_ms", detection_times)
            result.add_metric("average_detection_time_ms", avg_detection_time)
            result.add_metric("max_detection_time_ms", max_detection_time)
            result.add_metric("detection_rate_percent", detection_rate)
            result.add_metric("total_tests", total_tests)
            result.add_metric("claim_validation", {
                "paper_claim": "100% detection within 1ms",
                "achieved_rate": detection_rate,
                "achieved_time": avg_detection_time,
                "meets_claim": detection_rate == 100.0 and avg_detection_time <= 1.0
            })
            
        except Exception as e:
            result.set_error(f"Tamper detection benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Tamper Detection: {result.metrics.get('detection_rate_percent', 0):.1f}% in {result.metrics.get('average_detection_time_ms', 0):.2f}ms")
    
    def _benchmark_integrity_verification(self):
        """Benchmark integrity verification performance."""
        result = BenchmarkResult("Integrity Verification")
        result.start_time = time.time()
        
        try:
            verification_times = []
            file_sizes = []
            
            with tempfile.TemporaryDirectory() as tmpdir:
                for size_kb in [10, 100, 1000, 10000]:  # 10KB to 10MB
                    encoder = MAIFEncoder()
                    
                    # Create file of specific size
                    text_data = self._generate_random_text(size_kb * 1024)
                    encoder.add_text_block(text_data)
                    
                    maif_path = os.path.join(tmpdir, f"test_{size_kb}kb.maif")
                    manifest_path = os.path.join(tmpdir, f"test_{size_kb}kb_manifest.json")
                    encoder.build_maif(maif_path, manifest_path)
                    
                    file_size = os.path.getsize(maif_path)
                    file_sizes.append(file_size)
                    
                    # Benchmark verification
                    verify_start = time.time()
                    decoder = MAIFDecoder(maif_path, manifest_path)
                    is_valid = decoder.verify_integrity()
                    verify_end = time.time()
                    
                    verification_time = verify_end - verify_start
                    verification_times.append(verification_time)
                    
                    assert is_valid, f"Verification failed for {size_kb}KB file"
            
            # Calculate throughput
            throughputs = [size / time for size, time in zip(file_sizes, verification_times)]
            avg_throughput = statistics.mean(throughputs)
            
            result.add_metric("verification_times", verification_times)
            result.add_metric("file_sizes", file_sizes)
            result.add_metric("throughputs_bps", throughputs)
            result.add_metric("average_throughput_mbps", avg_throughput / (1024 * 1024))
            
        except Exception as e:
            result.set_error(f"Integrity verification benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Integrity Verification: {result.metrics.get('average_throughput_mbps', 0):.1f} MB/s")
    
    def _benchmark_multimodal_integration(self):
        """Benchmark multimodal data integration capabilities."""
        result = BenchmarkResult("Multimodal Integration")
        result.start_time = time.time()
        
        try:
            encoder = MAIFEncoder()
            
            # Add different data types
            text_hash = encoder.add_text_block("Sample text content")
            binary_hash = encoder.add_binary_block(b"Binary data content", "binary_data")
            embeddings_hash = encoder.add_embeddings_block([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
            
            # Test cross-modal relationships
            with tempfile.TemporaryDirectory() as tmpdir:
                maif_path = os.path.join(tmpdir, "multimodal.maif")
                manifest_path = os.path.join(tmpdir, "multimodal_manifest.json")
                encoder.build_maif(maif_path, manifest_path)
                
                # Verify all data types can be retrieved
                decoder = MAIFDecoder(maif_path, manifest_path)
                try:
                    texts = decoder.get_text_blocks()
                    embeddings = decoder.get_embeddings()
                except Exception:
                    # Handle UTF-8 errors gracefully
                    texts = []
                    embeddings = []
                
                result.add_metric("text_blocks_count", len(texts))
                result.add_metric("embeddings_count", len(embeddings))
                result.add_metric("total_blocks", len(decoder.blocks))
                result.add_metric("multimodal_support", True)
                
        except Exception as e:
            result.set_error(f"Multimodal integration benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Multimodal Integration: {result.metrics.get('total_blocks', 0)} blocks")
    
    def _benchmark_provenance_chains(self):
        """Benchmark provenance chain functionality."""
        result = BenchmarkResult("Provenance Chains")
        result.start_time = time.time()
        
        try:
            signer = MAIFSigner()
            
            # Create provenance chain
            chain_length = 100
            for i in range(chain_length):
                block_hash = hashlib.sha256(f"block_{i}".encode()).hexdigest()
                signer.add_provenance_entry(f"action_{i}", block_hash)
            
            # Verify chain integrity
            verifier = MAIFVerifier()
            chain_valid = True
            
            for i, entry in enumerate(signer.provenance_chain):
                if i > 0:
                    # Verify linkage to previous entry
                    prev_entry = signer.provenance_chain[i-1]
                    expected_hash = hashlib.sha256(
                        json.dumps(prev_entry.to_dict(), sort_keys=True).encode()
                    ).hexdigest()
                    if entry.previous_hash != expected_hash:
                        chain_valid = False
                        break
            
            result.add_metric("chain_length", chain_length)
            result.add_metric("chain_valid", chain_valid)
            result.add_metric("immutable_provenance", chain_valid)
            
        except Exception as e:
            result.set_error(f"Provenance chains benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Provenance Chains: {result.metrics.get('chain_length', 0)} entries")
    
    def _benchmark_privacy_features(self):
        """Benchmark privacy-by-design features."""
        result = BenchmarkResult("Privacy Features")
        result.start_time = time.time()
        
        try:
            # Test encryption
            privacy_engine = PrivacyEngine()
            test_data = b"Sensitive data that needs protection"
            
            encrypted_data, metadata = privacy_engine.encrypt_data(
                test_data, "test_block", EncryptionMode.AES_GCM
            )
            
            decrypted_data = privacy_engine.decrypt_data(
                encrypted_data, "test_block", metadata
            )
            
            # Test anonymization
            text_data = "John Smith works at ACME Corp and lives in New York"
            anonymized = privacy_engine.anonymize_data(text_data, "text_block")
            
            result.add_metric("encryption_successful", decrypted_data == test_data)
            result.add_metric("anonymization_applied", anonymized != text_data)
            result.add_metric("privacy_by_design", True)
            
        except Exception as e:
            result.set_error(f"Privacy features benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Privacy Features: Encryption & Anonymization")
    
    def _benchmark_repair_capabilities(self):
        """Benchmark automated repair - Paper claims 95%+ success rate."""
        result = BenchmarkResult("Repair Capabilities")
        result.start_time = time.time()
        
        try:
            repair_successes = 0
            total_repairs = 50
            
            with tempfile.TemporaryDirectory() as tmpdir:
                for test_num in range(total_repairs):
                    # Create test file
                    encoder = MAIFEncoder()
                    encoder.add_text_block(f"Test content {test_num}")
                    
                    maif_path = os.path.join(tmpdir, f"repair_test_{test_num}.maif")
                    manifest_path = os.path.join(tmpdir, f"repair_test_{test_num}_manifest.json")
                    encoder.build_maif(maif_path, manifest_path)
                    
                    # Introduce minor corruption (simulate recoverable errors)
                    if test_num % 10 != 0:  # 90% should be repairable
                        with open(manifest_path, 'r') as f:
                            manifest = json.load(f)
                        
                        # Corrupt a non-critical field
                        manifest['created'] = manifest['created'] + 1
                        
                        with open(manifest_path, 'w') as f:
                            json.dump(manifest, f)
                    
                    # Attempt repair
                    try:
                        repair_tool = MAIFRepairTool()
                        repair_success = repair_tool.repair_file(maif_path, manifest_path)
                        if repair_success:
                            repair_successes += 1
                    except:
                        pass  # Repair failed
            
            repair_rate = (repair_successes / total_repairs) * 100
            
            result.add_metric("total_repair_attempts", total_repairs)
            result.add_metric("successful_repairs", repair_successes)
            result.add_metric("repair_success_rate", repair_rate)
            result.add_metric("claim_validation", {
                "paper_claim": "95%+ automated repair success",
                "achieved": repair_rate,
                "meets_claim": repair_rate >= 95.0
            })
            
        except Exception as e:
            result.set_error(f"Repair capabilities benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Repair Capabilities: {result.metrics.get('repair_success_rate', 0):.1f}% success")
    
    def _benchmark_scalability(self):
        """Benchmark scalability with large datasets."""
        result = BenchmarkResult("Scalability")
        result.start_time = time.time()
        
        try:
            scalability_results = {}
            
            for block_count in [100, 1000, 10000]:
                encoder = MAIFEncoder()
                
                # Add many blocks
                encode_start = time.time()
                for i in range(block_count):
                    encoder.add_text_block(f"Block {i} content")
                encode_end = time.time()
                
                # Build file
                with tempfile.TemporaryDirectory() as tmpdir:
                    maif_path = os.path.join(tmpdir, f"scale_{block_count}.maif")
                    manifest_path = os.path.join(tmpdir, f"scale_{block_count}_manifest.json")
                    
                    build_start = time.time()
                    encoder.build_maif(maif_path, manifest_path)
                    build_end = time.time()
                    
                    file_size = os.path.getsize(maif_path)
                    
                    # Test parsing
                    parse_start = time.time()
                    decoder = MAIFDecoder(maif_path, manifest_path)
                    try:
                        texts = decoder.get_text_blocks()
                    except Exception:
                        texts = []  # Handle UTF-8 errors gracefully
                    parse_end = time.time()
                    
                    scalability_results[block_count] = {
                        "encode_time": encode_end - encode_start,
                        "build_time": build_end - build_start,
                        "parse_time": parse_end - parse_start,
                        "file_size": file_size,
                        "blocks_retrieved": len(texts)
                    }
            
            result.add_metric("scalability_results", scalability_results)
            result.add_metric("max_blocks_tested", max(scalability_results.keys()))
            
        except Exception as e:
            result.set_error(f"Scalability benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Scalability: Up to {result.metrics.get('max_blocks_tested', 0)} blocks")
    
    def _benchmark_concurrent_read_write(self):
        """Benchmark simultaneous read/write operations on the same file."""
        result = BenchmarkResult("Concurrent Read/Write")
        result.start_time = time.time()
        
        try:
            import threading
            import tempfile
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            # Create a test MAIF file
            encoder = MAIFEncoder(agent_id="concurrent_test")
            for i in range(100):
                encoder.add_text_block(f"Test block {i}", metadata={"block_id": i})
            
            with tempfile.NamedTemporaryFile(suffix='.maif', delete=False) as tmp_file:
                maif_path = tmp_file.name
            manifest_path = maif_path.replace('.maif', '_manifest.json')
            
            encoder.build_maif(maif_path, manifest_path)
            
            # Metrics collection
            read_times = []
            write_times = []
            errors = []
            
            def read_operation(reader_id):
                """Read operation that measures performance."""
                try:
                    start_time = time.time()
                    decoder = MAIFDecoder(maif_path, manifest_path)
                    blocks_read = 0
                    for block in decoder.blocks:
                        if block.block_type == "TEXT":
                            data = decoder.get_block_data(block.block_id)
                            blocks_read += 1
                    end_time = time.time()
                    read_times.append((end_time - start_time, blocks_read))
                except Exception as e:
                    errors.append(f"Read error {reader_id}: {str(e)}")
            
            def write_operation(writer_id):
                """Write operation that modifies the SAME file being read."""
                import os
                try:
                    start_time = time.time()
                    
                    # Read existing content from the SAME file
                    temp_decoder = MAIFDecoder(maif_path, manifest_path)
                    
                    # Create new encoder with existing content plus new data
                    new_encoder = MAIFEncoder(agent_id=f"writer_{writer_id}")
                    
                    # Copy existing blocks
                    for block in temp_decoder.blocks:
                        if block.block_type == "TEXT":
                            existing_data = temp_decoder.get_block_data(block.block_id)
                            new_encoder.add_text_block(existing_data.decode('utf-8'), metadata=block.metadata)
                    
                    # Add new blocks to create write activity
                    for i in range(3):  # Reduced for real contention
                        new_encoder.add_text_block(f"Writer {writer_id} block {i}")
                    
                    # Write back to SAME file (creates real contention)
                    temp_path = maif_path + f".tmp_writer_{writer_id}"
                    temp_manifest = temp_path.replace('.maif', '_manifest.json')
                    new_encoder.build_maif(temp_path, temp_manifest)
                    
                    # Atomic replace to modify the file being read
                    if os.path.exists(temp_path):
                        os.replace(temp_path, maif_path)
                        os.replace(temp_manifest, manifest_path)
                    
                    end_time = time.time()
                    write_times.append(end_time - start_time)
                except Exception as e:
                    errors.append(f"Write error {writer_id}: {str(e)}")
                    # Still record timing even for failed attempts
                    end_time = time.time()
                    write_times.append(end_time - start_time)
            
            # Run concurrent operations
            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = []
                
                # Submit 4 readers and 4 writers
                for i in range(4):
                    futures.append(executor.submit(read_operation, i))
                    futures.append(executor.submit(write_operation, i))
                
                # Wait for completion
                for future in as_completed(futures):
                    future.result()
            
            # Calculate metrics
            avg_read_time = statistics.mean([t[0] for t in read_times]) if read_times else 0
            avg_write_time = statistics.mean(write_times) if write_times else 0
            total_blocks_read = sum([t[1] for t in read_times])
            
            result.add_metric("concurrent_readers", 4)
            result.add_metric("concurrent_writers", 4)
            result.add_metric("average_read_time_ms", avg_read_time * 1000)
            result.add_metric("average_write_time_ms", avg_write_time * 1000)
            result.add_metric("total_blocks_read", total_blocks_read)
            result.add_metric("error_count", len(errors))
            result.add_metric("errors", errors)
            
            # Cleanup
            import os
            try:
                os.unlink(maif_path)
                os.unlink(manifest_path)
                for i in range(4):
                    writer_path = maif_path.replace('.maif', f'_writer_{i}.maif')
                    writer_manifest = writer_path.replace('.maif', '_manifest.json')
                    if os.path.exists(writer_path):
                        os.unlink(writer_path)
                    if os.path.exists(writer_manifest):
                        os.unlink(writer_manifest)
            except:
                pass
                
        except Exception as e:
            result.set_error(f"Concurrent read/write benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        print(f"✓ Concurrent Read/Write: {result.metrics.get('error_count', 'N/A')} errors")
    
    def _benchmark_read_during_write(self):
        """Benchmark read performance during concurrent write operations."""
        result = BenchmarkResult("Read Performance During Write")
        result.start_time = time.time()
        
        try:
            import threading
            import tempfile
            from concurrent.futures import ThreadPoolExecutor
            
            # Create initial test file
            encoder = MAIFEncoder(agent_id="read_write_test")
            for i in range(50):
                encoder.add_text_block(f"Initial block {i}")
            
            with tempfile.NamedTemporaryFile(suffix='.maif', delete=False) as tmp_file:
                maif_path = tmp_file.name
            manifest_path = maif_path.replace('.maif', '_manifest.json')
            encoder.build_maif(maif_path, manifest_path)
            
            read_times_baseline = []
            read_times_during_write = []
            
            def measure_read_performance(times_list):
                """Measure read performance and store in provided list."""
                for _ in range(10):
                    start_time = time.time()
                    decoder = MAIFDecoder(maif_path, manifest_path)
                    blocks_read = 0
                    for block in decoder.blocks:
                        if block.block_type == "TEXT":
                            data = decoder.get_block_data(block.block_id)
                            blocks_read += 1
                    end_time = time.time()
                    times_list.append(end_time - start_time)
            
            def continuous_write_operations():
                """Continuously modify the SAME file to create real contention."""
                import os
                for i in range(10):  # Reduced iterations for real file operations
                    try:
                        # Read existing file
                        temp_decoder = MAIFDecoder(maif_path, manifest_path)
                        
                        # Create new encoder with existing content plus new data
                        writer = MAIFEncoder(agent_id=f"background_writer_{i}")
                        
                        # Copy existing blocks
                        for block in temp_decoder.blocks:
                            if block.block_type == "TEXT":
                                existing_data = temp_decoder.get_block_data(block.block_id)
                                writer.add_text_block(existing_data.decode('utf-8'), metadata=block.metadata)
                        
                        # Add new block to create write activity
                        writer.add_text_block(f"Background write {i}")
                        
                        # Write back to same file (this creates real contention)
                        temp_path = maif_path + f".tmp_{i}"
                        temp_manifest = temp_path.replace('.maif', '_manifest.json')
                        writer.build_maif(temp_path, temp_manifest)
                        
                        # Atomic replace to simulate real concurrent modification
                        if os.path.exists(temp_path):
                            os.replace(temp_path, maif_path)
                            os.replace(temp_manifest, manifest_path)
                        
                        time.sleep(0.02)  # Small delay between modifications
                    except Exception as e:
                        # Expected - concurrent access will cause some failures
                        pass
            
            # Baseline read performance (no concurrent writes)
            measure_read_performance(read_times_baseline)
            
            # Read performance during concurrent writes
            with ThreadPoolExecutor(max_workers=4) as executor:
                # Start background write operations
                write_future = executor.submit(continuous_write_operations)
                
                # Measure read performance during writes
                time.sleep(0.1)  # Let writes start
                measure_read_performance(read_times_during_write)
                
                # Wait for writes to complete
                write_future.result()
            
            # Calculate performance impact
            baseline_avg = statistics.mean(read_times_baseline) * 1000
            during_write_avg = statistics.mean(read_times_during_write) * 1000
            performance_impact = ((during_write_avg - baseline_avg) / baseline_avg) * 100
            
            result.add_metric("baseline_read_time_ms", baseline_avg)
            result.add_metric("read_time_during_write_ms", during_write_avg)
            result.add_metric("performance_impact_percent", performance_impact)
            result.add_metric("read_samples_baseline", len(read_times_baseline))
            result.add_metric("read_samples_during_write", len(read_times_during_write))
            
            # Cleanup background files
            import os
            try:
                os.unlink(maif_path)
                os.unlink(manifest_path)
                for i in range(20):
                    bg_path = maif_path.replace('.maif', f'_bg_{i}.maif')
                    bg_manifest = bg_path.replace('.maif', '_manifest.json')
                    if os.path.exists(bg_path):
                        os.unlink(bg_path)
                    if os.path.exists(bg_manifest):
                        os.unlink(bg_manifest)
            except:
                pass
                
        except Exception as e:
            result.set_error(f"Read during write benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        impact = result.metrics.get('performance_impact_percent', 0)
        print(f"✓ Read During Write: {impact:.1f}% impact")
    
    def _benchmark_write_during_read(self):
        """Benchmark write performance during concurrent read operations."""
        result = BenchmarkResult("Write Performance During Read")
        result.start_time = time.time()
        
        try:
            import threading
            import tempfile
            from concurrent.futures import ThreadPoolExecutor
            
            # Create test file for reading
            encoder = MAIFEncoder(agent_id="write_read_test")
            for i in range(100):
                encoder.add_text_block(f"Read target block {i}")
            
            with tempfile.NamedTemporaryFile(suffix='.maif', delete=False) as tmp_file:
                read_target_path = tmp_file.name
            read_manifest_path = read_target_path.replace('.maif', '_manifest.json')
            encoder.build_maif(read_target_path, read_manifest_path)
            
            write_times_baseline = []
            write_times_during_read = []
            
            def measure_write_performance(times_list, test_phase):
                """Measure write performance by modifying the SAME file being read."""
                import os
                for i in range(5):  # Reduced for real file contention
                    try:
                        start_time = time.time()
                        
                        # Read existing content from the SAME file being read
                        temp_decoder = MAIFDecoder(read_target_path, read_manifest_path)
                        
                        # Create new encoder with existing content plus modifications
                        writer = MAIFEncoder(agent_id=f"write_modifier_{test_phase}_{i}")
                        
                        # Copy existing blocks
                        for block in temp_decoder.blocks:
                            if block.block_type == "TEXT":
                                existing_data = temp_decoder.get_block_data(block.block_id)
                                writer.add_text_block(existing_data.decode('utf-8'), metadata=block.metadata)
                        
                        # Add new block to create write activity on SAME file
                        writer.add_text_block(f"Write modification {test_phase}_{i}")
                        
                        # Write back to SAME file (creates real contention with readers)
                        temp_path = read_target_path + f".tmp_write_{i}"
                        temp_manifest = temp_path.replace('.maif', '_manifest.json')
                        writer.build_maif(temp_path, temp_manifest)
                        
                        # Atomic replace to modify the file being read
                        if os.path.exists(temp_path):
                            os.replace(temp_path, read_target_path)
                            os.replace(temp_manifest, read_manifest_path)
                        
                        end_time = time.time()
                        times_list.append(end_time - start_time)
                        
                        time.sleep(0.02)  # Small delay between modifications
                    except Exception as e:
                        # Expected - concurrent access will cause some failures
                        end_time = time.time()
                        times_list.append(end_time - start_time)  # Include failed attempts
            
            def continuous_read_operations():
                """Continuously read from the target file to simulate read load."""
                for _ in range(50):
                    decoder = MAIFDecoder(read_target_path, read_manifest_path)
                    for block in decoder.blocks:
                        if block.block_type == "TEXT":
                            data = decoder.get_block_data(block.block_id)
                    time.sleep(0.01)
            
            # Baseline write performance (no concurrent reads)
            measure_write_performance(write_times_baseline, "baseline")
            
            # Write performance during concurrent reads
            with ThreadPoolExecutor(max_workers=4) as executor:
                # Start background read operations
                read_future = executor.submit(continuous_read_operations)
                
                # Measure write performance during reads
                time.sleep(0.1)  # Let reads start
                measure_write_performance(write_times_during_read, "during_read")
                
                # Wait for reads to complete
                read_future.result()
            
            # Calculate performance impact
            baseline_avg = statistics.mean(write_times_baseline) * 1000
            during_read_avg = statistics.mean(write_times_during_read) * 1000
            performance_impact = ((during_read_avg - baseline_avg) / baseline_avg) * 100
            
            result.add_metric("baseline_write_time_ms", baseline_avg)
            result.add_metric("write_time_during_read_ms", during_read_avg)
            result.add_metric("performance_impact_percent", performance_impact)
            result.add_metric("write_samples_baseline", len(write_times_baseline))
            result.add_metric("write_samples_during_read", len(write_times_during_read))
            
            # Cleanup
            import os
            try:
                os.unlink(read_target_path)
                os.unlink(read_manifest_path)
                for prefix in ["baseline", "during_read"]:
                    for i in range(10):
                        write_path = read_target_path.replace('.maif', f'_{prefix}_{i}.maif')
                        write_manifest = write_path.replace('.maif', '_manifest.json')
                        if os.path.exists(write_path):
                            os.unlink(write_path)
                        if os.path.exists(write_manifest):
                            os.unlink(write_manifest)
            except:
                pass
                
        except Exception as e:
            result.set_error(f"Write during read benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        impact = result.metrics.get('performance_impact_percent', 0)
        print(f"✓ Write During Read: {impact:.1f}% impact")
    
    def _benchmark_lock_contention(self):
        """Benchmark lock contention under high concurrent access."""
        result = BenchmarkResult("Lock Contention Analysis")
        result.start_time = time.time()
        
        try:
            import threading
            import tempfile
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            # Create test file
            encoder = MAIFEncoder(agent_id="lock_test")
            for i in range(20):
                encoder.add_text_block(f"Lock test block {i}")
            
            with tempfile.NamedTemporaryFile(suffix='.maif', delete=False) as tmp_file:
                maif_path = tmp_file.name
            manifest_path = maif_path.replace('.maif', '_manifest.json')
            encoder.build_maif(maif_path, manifest_path)
            
            # Test different levels of concurrency
            concurrency_levels = [1, 2, 4, 8, 16]
            contention_results = {}
            
            def access_operation(op_id):
                """Perform read operation and measure time."""
                start_time = time.time()
                decoder = MAIFDecoder(maif_path, manifest_path)
                blocks_accessed = 0
                for block in decoder.blocks:
                    if block.block_type == "TEXT":
                        data = decoder.get_block_data(block.block_id)
                        blocks_accessed += 1
                end_time = time.time()
                return end_time - start_time, blocks_accessed
            
            for concurrency in concurrency_levels:
                operation_times = []
                
                with ThreadPoolExecutor(max_workers=concurrency) as executor:
                    # Submit operations
                    futures = [executor.submit(access_operation, i) for i in range(concurrency * 3)]
                    
                    # Collect results
                    for future in as_completed(futures):
                        op_time, blocks = future.result()
                        operation_times.append(op_time)
                
                avg_time = statistics.mean(operation_times) * 1000
                max_time = max(operation_times) * 1000
                min_time = min(operation_times) * 1000
                
                contention_results[concurrency] = {
                    "average_time_ms": avg_time,
                    "max_time_ms": max_time,
                    "min_time_ms": min_time,
                    "operations_count": len(operation_times)
                }
            
            # Calculate contention impact
            baseline_time = contention_results[1]["average_time_ms"]
            max_contention_time = contention_results[max(concurrency_levels)]["average_time_ms"]
            contention_impact = ((max_contention_time - baseline_time) / baseline_time) * 100
            
            result.add_metric("contention_results", contention_results)
            result.add_metric("baseline_time_ms", baseline_time)
            result.add_metric("max_contention_time_ms", max_contention_time)
            result.add_metric("contention_impact_percent", contention_impact)
            result.add_metric("max_concurrency_tested", max(concurrency_levels))
            
            # Cleanup
            import os
            try:
                os.unlink(maif_path)
                os.unlink(manifest_path)
            except:
                pass
                
        except Exception as e:
            result.set_error(f"Lock contention benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        impact = result.metrics.get('contention_impact_percent', 0)
        print(f"✓ Lock Contention: {impact:.1f}% impact at max concurrency")
    
    def _benchmark_concurrent_block_access(self):
        """Benchmark concurrent access to different blocks within the same file."""
        result = BenchmarkResult("Concurrent Block Access")
        result.start_time = time.time()
        
        try:
            import threading
            import tempfile
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            # Create test file with many blocks
            encoder = MAIFEncoder(agent_id="block_access_test")
            block_count = 100
            for i in range(block_count):
                encoder.add_text_block(f"Block {i} content with some data to make it substantial",
                                     metadata={"block_index": i})
            
            with tempfile.NamedTemporaryFile(suffix='.maif', delete=False) as tmp_file:
                maif_path = tmp_file.name
            manifest_path = maif_path.replace('.maif', '_manifest.json')
            encoder.build_maif(maif_path, manifest_path)
            
            # Get block IDs for targeted access
            decoder = MAIFDecoder(maif_path, manifest_path)
            text_blocks = [block for block in decoder.blocks if block.block_type in ["text", "text_data", "TEXT"]]
            
            print(f"  Debug: Created {len(decoder.blocks)} total blocks, {len(text_blocks)} text blocks")
            if text_blocks:
                print(f"  Debug: First text block type: {text_blocks[0].block_type}, ID: {text_blocks[0].block_id}")
            
            access_times = []
            block_access_counts = {}
            
            def access_specific_blocks(worker_id, block_indices):
                """Access specific blocks and measure performance."""
                worker_times = []
                worker_access_counts = {}
                
                for block_idx in block_indices:
                    if block_idx < len(text_blocks):
                        start_time = time.time()
                        block = text_blocks[block_idx]
                        try:
                            data = decoder.get_block_data(block.block_id)  # Thread-safe!
                            end_time = time.time()
                            
                            # Record access time regardless of data content for performance measurement
                            access_time = end_time - start_time
                            worker_times.append(access_time)
                            
                            # Track which blocks are being accessed (thread-local)
                            worker_access_counts[block_idx] = worker_access_counts.get(block_idx, 0) + 1
                            
                        except Exception as e:
                            # Still record the attempt time even if it fails
                            end_time = time.time()
                            access_time = end_time - start_time
                            worker_times.append(access_time)
                            worker_access_counts[block_idx] = worker_access_counts.get(block_idx, 0) + 1
                
                return worker_times, worker_access_counts
            
            # Test concurrent access to different block ranges
            num_workers = 8
            blocks_per_worker = block_count // num_workers
            
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                futures = []
                
                for worker_id in range(num_workers):
                    start_idx = worker_id * blocks_per_worker
                    end_idx = min(start_idx + blocks_per_worker, block_count)
                    block_indices = list(range(start_idx, end_idx))
                    
                    future = executor.submit(access_specific_blocks, worker_id, block_indices)
                    futures.append(future)
                
                # Collect all access times and merge access counts
                for future in as_completed(futures):
                    worker_times, worker_access_counts = future.result()
                    access_times.extend(worker_times)
                    
                    # Merge worker access counts into global counts
                    for block_idx, count in worker_access_counts.items():
                        block_access_counts[block_idx] = block_access_counts.get(block_idx, 0) + count
            
            # Calculate metrics
            if access_times:
                avg_access_time = statistics.mean(access_times) * 1000
                max_access_time = max(access_times) * 1000
                min_access_time = min(access_times) * 1000
            else:
                avg_access_time = max_access_time = min_access_time = 0
                print(f"  Warning: No successful block accesses recorded")
            
            total_accesses = len(access_times)
            unique_blocks_accessed = len(block_access_counts)
            
            # Calculate throughput (use current time since end_time isn't set yet)
            current_time = time.time()
            total_time = current_time - result.start_time
            blocks_per_second = total_accesses / total_time if total_time > 0 and total_accesses > 0 else 0
            
            print(f"  Debug: {total_accesses} accesses in {total_time:.3f}s = {blocks_per_second:.1f} blocks/sec")
            
            result.add_metric("total_blocks_in_file", block_count)
            result.add_metric("concurrent_workers", num_workers)
            result.add_metric("total_block_accesses", total_accesses)
            result.add_metric("unique_blocks_accessed", unique_blocks_accessed)
            result.add_metric("average_access_time_ms", avg_access_time)
            result.add_metric("max_access_time_ms", max_access_time)
            result.add_metric("min_access_time_ms", min_access_time)
            result.add_metric("blocks_per_second", blocks_per_second)
            result.add_metric("block_access_distribution", dict(sorted(block_access_counts.items())))
            
            # Cleanup
            import os
            try:
                os.unlink(maif_path)
                os.unlink(manifest_path)
            except:
                pass
                
        except Exception as e:
            result.set_error(f"Concurrent block access benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        blocks_per_sec = result.metrics.get('blocks_per_second', 0)
        print(f"✓ Concurrent Block Access: {blocks_per_sec:.1f} blocks/sec")

    def _benchmark_novel_algorithms(self):
        """Benchmark all novel algorithms: ACAM, HSC, CSB, and optimized variants."""
        print("\n--- Novel Algorithms Performance ---")
        
        # Benchmark ACAM (Adaptive Cross-Modal Attention)
        self._benchmark_acam()
        
        # Benchmark HSC (Hierarchical Semantic Compression)
        self._benchmark_hsc()
        
        # Benchmark CSB (Cryptographic Semantic Binding)
        self._benchmark_csb()
        
        # Benchmark Enhanced Variants
        self._benchmark_enhanced_algorithms()
    
    def _benchmark_acam(self):
        """Benchmark Adaptive Cross-Modal Attention algorithm."""
        result = BenchmarkResult("ACAM - Adaptive Cross-Modal Attention")
        result.start_time = time.time()
        
        try:
            from maif.semantic_optimized import AdaptiveCrossModalAttention
            import numpy as np
            
            print("  Testing ACAM with multimodal data...")
            
            # Initialize ACAM
            acam = AdaptiveCrossModalAttention()
            
            # Create test multimodal data
            test_sizes = [10, 100, 1000]
            acam_times = []
            
            for size in test_sizes:
                # Generate test embeddings for different modalities
                text_embeddings = np.random.rand(size, 384)
                image_embeddings = np.random.rand(size, 384)
                audio_embeddings = np.random.rand(size, 384)
                
                embeddings = {
                    'text': text_embeddings,
                    'image': image_embeddings,
                    'audio': audio_embeddings
                }
                
                trust_scores = {
                    'text': 1.0,
                    'image': 0.8,
                    'audio': 0.7
                }
                
                # Benchmark attention computation
                start_time = time.time()
                attention_weights = acam.compute_attention_weights(embeddings, trust_scores)
                unified_repr = acam.get_attended_representation(embeddings, attention_weights, 'text')
                end_time = time.time()
                
                acam_time = (end_time - start_time) * 1000
                acam_times.append(acam_time)
                
                print(f"    Size {size}: {acam_time:.2f}ms")
            
            avg_time = statistics.mean(acam_times)
            result.add_metric("test_sizes", test_sizes)
            result.add_metric("processing_times_ms", acam_times)
            result.add_metric("average_time_ms", avg_time)
            result.add_metric("algorithm", "ACAM")
            
        except Exception as e:
            result.set_error(f"ACAM benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        avg_time = result.metrics.get('average_time_ms', 0)
        print(f"✓ ACAM: {avg_time:.1f}ms avg")
    
    def _benchmark_hsc(self):
        """Benchmark Hierarchical Semantic Compression algorithm."""
        result = BenchmarkResult("HSC - Hierarchical Semantic Compression")
        result.start_time = time.time()
        
        try:
            from maif.semantic_optimized import HierarchicalSemanticCompression
            import numpy as np
            
            print("  Testing HSC with embedding compression...")
            
            # Initialize HSC from semantic.py (has decompress_embeddings method)
            from maif.semantic import HierarchicalSemanticCompression
            hsc = HierarchicalSemanticCompression()
            
            # Test different embedding set sizes
            test_sizes = [100, 1000, 10000]
            compression_results = []
            
            for size in test_sizes:
                # Generate test embeddings
                embeddings = [np.random.rand(384).tolist() for _ in range(size)]
                
                # Benchmark compression
                start_time = time.time()
                compressed_result = hsc.compress_embeddings(embeddings, preserve_fidelity=True)
                compression_time = time.time() - start_time
                
                # Benchmark decompression
                start_time = time.time()
                decompressed = hsc.decompress_embeddings(compressed_result)
                decompression_time = time.time() - start_time
                
                # Calculate compression metrics
                original_size = len(embeddings) * 384 * 4  # 4 bytes per float
                compressed_size = len(str(compressed_result).encode())
                compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
                
                result_data = {
                    "size": size,
                    "compression_time_ms": compression_time * 1000,
                    "decompression_time_ms": decompression_time * 1000,
                    "compression_ratio": compression_ratio,
                    "fidelity_score": compressed_result.get("metadata", {}).get("fidelity_score", 0.0)
                }
                compression_results.append(result_data)
                
                print(f"    Size {size}: Compression {compression_time*1000:.2f}ms, Ratio {compression_ratio:.2f}x")
            
            avg_compression_time = statistics.mean([r["compression_time_ms"] for r in compression_results])
            avg_compression_ratio = statistics.mean([r["compression_ratio"] for r in compression_results])
            
            result.add_metric("test_sizes", test_sizes)
            result.add_metric("compression_results", compression_results)
            result.add_metric("average_compression_time_ms", avg_compression_time)
            result.add_metric("average_compression_ratio", avg_compression_ratio)
            result.add_metric("algorithm", "HSC")
            
        except Exception as e:
            result.set_error(f"HSC benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        avg_time = result.metrics.get('average_compression_time_ms', 0)
        avg_ratio = result.metrics.get('average_compression_ratio', 0)
        print(f"✓ HSC: {avg_time:.1f}ms avg, {avg_ratio:.1f}x ratio")
    
    def _benchmark_csb(self):
        """Benchmark Cryptographic Semantic Binding algorithm."""
        result = BenchmarkResult("CSB - Cryptographic Semantic Binding")
        result.start_time = time.time()
        
        try:
            from maif.semantic import CryptographicSemanticBinding
            import numpy as np
            
            print("  Testing CSB with semantic commitments...")
            
            # Initialize CSB
            csb = CryptographicSemanticBinding()
            
            # Test different embedding sizes
            test_sizes = [384, 768, 1536]  # Different embedding dimensions
            csb_results = []
            
            for dim in test_sizes:
                # Generate test embedding and source data
                embedding = np.random.rand(dim).tolist()
                source_data = f"Test source data for dimension {dim}"
                
                # Benchmark commitment creation
                start_time = time.time()
                commitment = csb.create_semantic_commitment(embedding, source_data)
                commitment_time = time.time() - start_time
                
                # Benchmark zero-knowledge proof creation
                start_time = time.time()
                zk_proof = csb.create_zero_knowledge_proof(embedding, commitment)
                proof_time = time.time() - start_time
                
                # Benchmark verification
                start_time = time.time()
                is_valid = csb.verify_semantic_binding(embedding, source_data, commitment)
                verification_time = time.time() - start_time
                
                result_data = {
                    "dimension": dim,
                    "commitment_time_ms": commitment_time * 1000,
                    "proof_time_ms": proof_time * 1000,
                    "verification_time_ms": verification_time * 1000,
                    "verification_success": is_valid
                }
                csb_results.append(result_data)
                
                print(f"    Dim {dim}: Commit {commitment_time*1000:.2f}ms, Verify {verification_time*1000:.2f}ms")
            
            avg_commitment_time = statistics.mean([r["commitment_time_ms"] for r in csb_results])
            avg_verification_time = statistics.mean([r["verification_time_ms"] for r in csb_results])
            
            result.add_metric("test_dimensions", test_sizes)
            result.add_metric("csb_results", csb_results)
            result.add_metric("average_commitment_time_ms", avg_commitment_time)
            result.add_metric("average_verification_time_ms", avg_verification_time)
            result.add_metric("algorithm", "CSB")
            
        except Exception as e:
            result.set_error(f"CSB benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        commit_time = result.metrics.get('average_commitment_time_ms', 0)
        verify_time = result.metrics.get('average_verification_time_ms', 0)
        print(f"✓ CSB: {commit_time:.1f}ms commit, {verify_time:.1f}ms verify")
    
    def _benchmark_enhanced_algorithms(self):
        """Benchmark enhanced/optimized variants of novel algorithms."""
        result = BenchmarkResult("Enhanced Novel Algorithms")
        result.start_time = time.time()
        
        try:
            print("  Testing enhanced algorithm variants...")
            
            # Test OptimizedSemanticEmbedder
            from maif.semantic_optimized import OptimizedSemanticEmbedder
            
            embedder = OptimizedSemanticEmbedder()
            
            # Test batch embedding generation
            test_texts = [f"Test document {i} with semantic content" for i in range(1000)]
            
            start_time = time.time()
            embeddings = embedder.embed_texts_batch(test_texts, batch_size=64)
            batch_time = time.time() - start_time
            
            # Test FAISS indexing if available
            faiss_time = 0
            if hasattr(embedder, 'build_search_index'):
                start_time = time.time()
                embedder.build_search_index(embeddings)
                faiss_time = time.time() - start_time
            
            # Test fast similarity search
            query_embedding = embeddings[0] if embeddings else None
            search_time = 0
            if query_embedding and hasattr(embedder, 'search_similar_fast'):
                start_time = time.time()
                similar = embedder.search_similar_fast(query_embedding, top_k=10)
                search_time = time.time() - start_time
            
            result.add_metric("batch_embedding_time_ms", batch_time * 1000)
            result.add_metric("faiss_index_time_ms", faiss_time * 1000)
            result.add_metric("fast_search_time_ms", search_time * 1000)
            result.add_metric("embeddings_generated", len(embeddings))
            result.add_metric("algorithm", "Enhanced_Variants")
            
            print(f"    Batch embedding: {batch_time*1000:.2f}ms for {len(embeddings)} texts")
            if faiss_time > 0:
                print(f"    FAISS indexing: {faiss_time*1000:.2f}ms")
            if search_time > 0:
                print(f"    Fast search: {search_time*1000:.2f}ms")
            
        except Exception as e:
            result.set_error(f"Enhanced algorithms benchmark failed: {str(e)}")
        
        result.end_time = time.time()
        self.results.append(result)
        batch_time = result.metrics.get('batch_embedding_time_ms', 0)
        print(f"✓ Enhanced Algorithms: {batch_time:.1f}ms batch processing")

    def _generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive benchmark report."""
        print("\n" + "="*80)
        print("BENCHMARK RESULTS SUMMARY")
        print("="*80)
        
        report = {
            "timestamp": time.time(),
            "total_benchmarks": len(self.results),
            "successful_benchmarks": sum(1 for r in self.results if r.success),
            "failed_benchmarks": sum(1 for r in self.results if not r.success),
            "results": {},
            "paper_claims_validation": {},
            "overall_assessment": {}
        }
        
        # Process results
        claims_met = 0
        total_claims = 0
        
        for result in self.results:
            report["results"][result.name] = {
                "success": result.success,
                "duration": result.duration(),
                "metrics": result.metrics,
                "error": result.error_message if not result.success else None
            }
            
            # Check claim validation
            if "claim_validation" in result.metrics:
                claim_info = result.metrics["claim_validation"]
                report["paper_claims_validation"][result.name] = claim_info
                total_claims += 1
                if claim_info.get("meets_claim", False):
                    claims_met += 1
            
            # Print result summary
            status = "✓ PASS" if result.success else "✗ FAIL"
            print(f"{status} {result.name}: {result.duration():.2f}s")
            if not result.success:
                print(f"    Error: {result.error_message}")
        
        # Overall assessment
        claims_percentage = (claims_met / total_claims * 100) if total_claims > 0 else 0
        report["overall_assessment"] = {
            "claims_met": claims_met,
            "total_claims": total_claims,
            "claims_percentage": claims_percentage,
            "implementation_maturity": self._assess_maturity(report)
        }
        
        print(f"\nPaper Claims Validation: {claims_met}/{total_claims} ({claims_percentage:.1f}%)")
        print(f"Overall Implementation Status: {report['overall_assessment']['implementation_maturity']}")
        
        # Save detailed report
        report_path = self.output_dir / "benchmark_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nDetailed report saved to: {report_path}")
        return report
    
    def _assess_maturity(self, report: Dict[str, Any]) -> str:
        """Assess implementation maturity based on benchmark results."""
        successful = report["successful_benchmarks"]
        total = report["total_benchmarks"]
        success_rate = successful / total if total > 0 else 0
        
        claims_validation = report["paper_claims_validation"]
        claims_met = sum(1 for claim in claims_validation.values() if claim.get("meets_claim", False))
        total_claims = len(claims_validation)
        claims_rate = claims_met / total_claims if total_claims > 0 else 0
        
        if success_rate >= 0.9 and claims_rate >= 0.8:
            return "Production Ready"
        elif success_rate >= 0.7 and claims_rate >= 0.6:
            return "Beta Quality"
        elif success_rate >= 0.5 and claims_rate >= 0.4:
            return "Alpha Quality"
        else:
            return "Prototype"
    
    # Helper methods for generating test data
    def _generate_lorem_ipsum(self, length: int) -> str:
        """Generate Lorem Ipsum text of specified length."""
        lorem = ("Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod "
                "tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, "
                "quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo "
                "consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse "
                "cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat "
                "non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ")
        
        result = ""
        while len(result) < length:
            result += lorem
        return result[:length]
    
    def _generate_test_json(self, entries: int) -> Dict[str, Any]:
        """Generate test JSON data with specified number of entries."""
        return {
            "metadata": {
                "version": "1.0",
                "created": time.time(),
                "entries": entries
            },
            "data": [
                {
                    "id": i,
                    "name": f"Item {i}",
                    "value": random.random(),
                    "tags": [f"tag{j}" for j in range(random.randint(1, 5))]
                }
                for i in range(entries)
            ]
        }
    
    def _generate_code_sample(self, length: int) -> str:
        """Generate sample code of specified length."""
        code_template = '''
def example_function(param1, param2):
    """Example function for testing."""
    result = param1 + param2
    if result > 100:
        return result * 2
    else:
        return result
    
class ExampleClass:
    def __init__(self, value):
        self.value = value
    
    def process(self):
        return self.value ** 2
'''
        result = ""
        while len(result) < length:
            result += code_template
        return result[:length]
    
    def _generate_random_text(self, length: int) -> str:
        """Generate random text of specified length."""
        import string
        chars = string.ascii_letters + string.digits + " \n"
        return ''.join(random.choice(chars) for _ in range(length))
    
    def _create_mock_video_data(self, format_type: str = "mp4", size_mb: float = 1.0) -> bytes:
        """Create mock video data for benchmarking."""
        import struct
        
        size_bytes = int(size_mb * 1024 * 1024)
        
        if format_type == "mp4":
            # Create minimal MP4 structure
            data = bytearray()
            
            # ftyp box
            ftyp_size = 32
            data.extend(struct.pack('>I', ftyp_size))
            data.extend(b'ftyp')
            data.extend(b'mp42')
            data.extend(struct.pack('>I', 0))
            data.extend(b'mp42isom')
            data.extend(b'\x00' * 12)
            
            # mvhd box with duration
            mvhd_size = 108
            data.extend(struct.pack('>I', mvhd_size))
            data.extend(b'mvhd')
            data.extend(struct.pack('>I', 0))  # version/flags
            data.extend(struct.pack('>I', 0))  # creation time
            data.extend(struct.pack('>I', 0))  # modification time
            data.extend(struct.pack('>I', 1000))  # timescale
            data.extend(struct.pack('>I', 30000))  # duration (30 seconds)
            data.extend(b'\x00' * 80)
            
            # Fill remaining space with dummy data
            remaining = size_bytes - len(data)
            if remaining > 0:
                data.extend(b'\x00' * remaining)
            
            return bytes(data[:size_bytes])
        
        else:
            # Generic video data
            return b'\x00' * size_bytes
    
    def _benchmark_video_storage_performance(self):
        """Benchmark video storage performance and metadata extraction."""
        result = BenchmarkResult("Video Storage Performance")
        
        try:
            print("\n--- Video Storage Performance ---")
            
            # Test different video sizes
            video_sizes = [1, 5, 10, 25]  # MB
            storage_times = []
            extraction_times = []
            
            for size_mb in video_sizes:
                print(f"  Testing {size_mb}MB video storage...")
                
                # Create mock video data
                video_data = self._create_mock_video_data("mp4", size_mb)
                
                # Test storage performance
                encoder = MAIFEncoder(enable_privacy=False)
                
                storage_start = time.time()
                video_hash = encoder.add_video_block(
                    video_data,
                    metadata={"title": f"Test Video {size_mb}MB"},
                    extract_metadata=True
                )
                storage_end = time.time()
                
                storage_time = (storage_end - storage_start) * 1000  # ms
                storage_times.append(storage_time)
                
                # Test metadata extraction time
                extraction_start = time.time()
                metadata = encoder.blocks[-1].metadata
                extraction_end = time.time()
                
                extraction_time = (extraction_end - extraction_start) * 1000  # ms
                extraction_times.append(extraction_time)
                
                print(f"    Storage: {storage_time:.2f}ms, Metadata extraction: {extraction_time:.2f}ms")
            
            # Calculate statistics
            avg_storage_time = statistics.mean(storage_times)
            avg_extraction_time = statistics.mean(extraction_times)
            
            # Calculate throughput (MB/s)
            total_data_mb = sum(video_sizes)
            total_time_s = sum(storage_times) / 1000
            throughput_mbs = total_data_mb / total_time_s if total_time_s > 0 else 0
            
            result.add_metric("video_sizes_mb", video_sizes)
            result.add_metric("storage_times_ms", storage_times)
            result.add_metric("extraction_times_ms", extraction_times)
            result.add_metric("average_storage_time_ms", avg_storage_time)
            result.add_metric("average_extraction_time_ms", avg_extraction_time)
            result.add_metric("storage_throughput_mbs", throughput_mbs)
            
            result.add_metric("claim_validation", {
                "video_storage_claim": "Efficient video storage with metadata extraction",
                "achieved_throughput": throughput_mbs,
                "average_storage_time": avg_storage_time,
                "meets_expectation": throughput_mbs > 10.0  # Expect >10 MB/s
            })
            
        except Exception as e:
            result.set_error(f"Video storage benchmark failed: {str(e)}")
        
        self.results.append(result)
        print(f"✓ Video Storage: {result.metrics.get('storage_throughput_mbs', 0):.1f} MB/s throughput")
    
    def _benchmark_video_metadata_extraction(self):
        """Benchmark video metadata extraction accuracy and performance."""
        result = BenchmarkResult("Video Metadata Extraction")
        
        try:
            print("\n--- Video Metadata Extraction ---")
            
            # Test different video formats and properties
            test_videos = [
                {"format": "mp4", "duration": 10.0, "width": 1920, "height": 1080},
                {"format": "mp4", "duration": 30.0, "width": 1280, "height": 720},
                {"format": "mp4", "duration": 60.0, "width": 3840, "height": 2160},
            ]
            
            extraction_times = []
            accuracy_scores = []
            
            for i, video_props in enumerate(test_videos):
                print(f"  Testing video {i+1}: {video_props['width']}x{video_props['height']}, {video_props['duration']}s")
                
                # Create mock video with proper headers for accurate metadata extraction
                video_data = self._create_realistic_mock_video_data(video_props)
                
                encoder = MAIFEncoder(enable_privacy=False)
                
                # Time metadata extraction
                extraction_start = time.time()
                encoder.add_video_block(video_data, extract_metadata=True)
                extraction_end = time.time()
                
                extraction_time = (extraction_end - extraction_start) * 1000
                extraction_times.append(extraction_time)
                
                # Check extraction accuracy
                metadata = encoder.blocks[-1].metadata
                accuracy = 0
                
                # Check format detection
                if metadata.get("format") == video_props["format"]:
                    accuracy += 1
                
                # Check if duration was extracted (may not match exactly with mock data)
                if metadata.get("duration") is not None:
                    accuracy += 1
                
                # Check if resolution was extracted
                if metadata.get("resolution"):
                    accuracy += 1
                
                # Check semantic analysis
                if metadata.get("has_semantic_analysis"):
                    accuracy += 1
                
                accuracy_score = accuracy / 4.0  # 4 possible checks
                accuracy_scores.append(accuracy_score)
                
                print(f"    Extraction time: {extraction_time:.2f}ms, Accuracy: {accuracy_score:.1%}")
            
            avg_extraction_time = statistics.mean(extraction_times)
            avg_accuracy = statistics.mean(accuracy_scores)
            
            result.add_metric("test_videos", test_videos)
            result.add_metric("extraction_times_ms", extraction_times)
            result.add_metric("accuracy_scores", accuracy_scores)
            result.add_metric("average_extraction_time_ms", avg_extraction_time)
            result.add_metric("average_accuracy", avg_accuracy)
            
            result.add_metric("claim_validation", {
                "metadata_extraction_claim": "Accurate video metadata extraction",
                "achieved_accuracy": avg_accuracy,
                "average_extraction_time": avg_extraction_time,
                "meets_expectation": avg_accuracy > 0.5 and avg_extraction_time < 100
            })
            
        except Exception as e:
            result.set_error(f"Video metadata extraction benchmark failed: {str(e)}")
        
        self.results.append(result)
        print(f"✓ Video Metadata: {result.metrics.get('average_accuracy', 0):.1%} accuracy, {result.metrics.get('average_extraction_time_ms', 0):.1f}ms avg")
    
    def _benchmark_video_querying_performance(self):
        """Benchmark video querying and search performance."""
        result = BenchmarkResult("Video Querying Performance")
        
        try:
            print("\n--- Video Querying Performance ---")
            
            # Create a large collection of test videos for stress testing
            encoder = MAIFEncoder(enable_privacy=False)
            video_count = 5000  # 5k videos for reasonable testing
            
            print(f"  Creating {video_count:,} test videos...")
            
            # Add videos with different properties
            for i in range(video_count):
                video_data = self._create_mock_video_data("mp4", 0.5)  # Small videos for speed
                
                # Vary video properties
                duration = random.uniform(10, 300)  # 10s to 5min
                width = random.choice([1280, 1920, 3840])
                height = random.choice([720, 1080, 2160])
                
                encoder.add_video_block(
                    video_data,
                    metadata={
                        "title": f"Test Video {i}",
                        "duration": duration,
                        "resolution": f"{width}x{height}",
                        "format": "mp4",
                        "tags": [f"tag{i%10}", f"category{i%5}"]
                    },
                    extract_metadata=True  # Enable metadata extraction for semantic search
                )
            
            # Save to temporary file
            temp_dir = tempfile.mkdtemp()
            maif_path = os.path.join(temp_dir, "video_test.maif")
            manifest_path = os.path.join(temp_dir, "video_test.maif.manifest.json")
            
            encoder.build_maif(maif_path, manifest_path)
            
            # Test querying performance
            decoder = MAIFDecoder(maif_path, manifest_path)
            
            query_tests = [
                {"name": "Duration Range", "params": {"duration_range": (60, 180)}},
                {"name": "HD Resolution", "params": {"min_resolution": "1080p"}},
                {"name": "4K Resolution", "params": {"min_resolution": "4K"}},
                {"name": "Format Filter", "params": {"format_filter": "mp4"}},
                {"name": "Size Filter", "params": {"max_size_mb": 1.0}},
            ]
            
            query_times = []
            result_counts = []
            
            for query_test in query_tests:
                print(f"  Testing {query_test['name']} query...")
                
                query_start = time.time()
                results = decoder.query_videos(**query_test["params"])
                query_end = time.time()
                
                query_time = (query_end - query_start) * 1000
                query_times.append(query_time)
                result_counts.append(len(results))
                
                print(f"    Query time: {query_time:.2f}ms, Results: {len(results)}")
            
            # Test semantic search if available
            semantic_search_time = 0
            try:
                semantic_start = time.time()
                semantic_results = decoder.search_videos_by_content("test video", top_k=10)
                semantic_end = time.time()
                semantic_search_time = (semantic_end - semantic_start) * 1000
                print(f"  Semantic search: {semantic_search_time:.2f}ms, Results: {len(semantic_results)}")
            except Exception as e:
                print(f"  Semantic search: Failed - {str(e)}")
                semantic_search_time = -1  # Indicate failure
            
            avg_query_time = statistics.mean(query_times)
            total_results = sum(result_counts)
            
            result.add_metric("video_count", video_count)
            result.add_metric("query_tests", [test["name"] for test in query_tests])
            result.add_metric("query_times_ms", query_times)
            result.add_metric("result_counts", result_counts)
            result.add_metric("average_query_time_ms", avg_query_time)
            result.add_metric("semantic_search_time_ms", semantic_search_time)
            result.add_metric("total_results_found", total_results)
            
            result.add_metric("claim_validation", {
                "video_querying_claim": "Fast video querying and search",
                "achieved_avg_query_time": avg_query_time,
                "semantic_search_time": semantic_search_time,
                "meets_expectation": avg_query_time < 50.0  # Expect <50ms for queries
            })
            
            # Cleanup
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            
        except Exception as e:
            result.set_error(f"Video querying benchmark failed: {str(e)}")
        
        self.results.append(result)
        print(f"✓ Video Querying: {result.metrics.get('average_query_time_ms', 0):.1f}ms avg query time")


    def _create_realistic_mock_video_data(self, video_props: Dict) -> bytes:
        """Create mock video data with proper MP4 headers for accurate metadata extraction."""
        import struct
        
        # Create a minimal but valid MP4 structure
        # This will have proper headers that the metadata extractor can parse
        
        # ftyp box (file type)
        ftyp_data = b'ftypisom\x00\x00\x02\x00isomiso2avc1mp41'
        ftyp_size = len(ftyp_data) + 8
        ftyp_box = struct.pack('>I', ftyp_size) + b'ftyp' + ftyp_data
        
        # mvhd box (movie header) with duration and timescale
        timescale = 1000  # 1000 units per second
        duration_units = int(video_props["duration"] * timescale)
        
        mvhd_data = (
            b'\x00\x00\x00\x00'  # version and flags
            + b'\x00\x00\x00\x00'  # creation time
            + b'\x00\x00\x00\x00'  # modification time
            + struct.pack('>I', timescale)  # timescale
            + struct.pack('>I', duration_units)  # duration
            + b'\x00\x01\x00\x00'  # preferred rate (1.0)
            + b'\x01\x00\x00\x00'  # preferred volume (1.0)
            + b'\x00' * 10  # reserved
            + b'\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x40\x00\x00\x00'  # matrix
            + b'\x00' * 24  # pre_defined
            + b'\x00\x00\x00\x02'  # next_track_ID
        )
        mvhd_size = len(mvhd_data) + 8
        mvhd_box = struct.pack('>I', mvhd_size) + b'mvhd' + mvhd_data
        
        # tkhd box (track header) with video dimensions
        width = video_props["width"]
        height = video_props["height"]
        
        tkhd_data = (
            b'\x00\x00\x00\x07'  # version and flags (track enabled)
            + b'\x00\x00\x00\x00'  # creation time
            + b'\x00\x00\x00\x00'  # modification time
            + b'\x00\x00\x00\x01'  # track ID
            + b'\x00\x00\x00\x00'  # reserved
            + struct.pack('>I', duration_units)  # duration
            + b'\x00' * 8  # reserved
            + b'\x00\x00'  # layer
            + b'\x00\x00'  # alternate_group
            + b'\x00\x00'  # volume
            + b'\x00\x00'  # reserved
            + b'\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x40\x00\x00\x00'  # matrix
            + struct.pack('>I', width << 16)  # width (fixed point)
            + struct.pack('>I', height << 16)  # height (fixed point)
        )
        tkhd_size = len(tkhd_data) + 8
        tkhd_box = struct.pack('>I', tkhd_size) + b'tkhd' + tkhd_data
        
        # Combine boxes into a minimal moov box
        moov_content = mvhd_box + tkhd_box
        moov_size = len(moov_content) + 8
        moov_box = struct.pack('>I', moov_size) + b'moov' + moov_content
        
        # Create minimal mdat box with some dummy video data
        dummy_video_data = b'\x00' * 1024  # 1KB of dummy video data
        mdat_size = len(dummy_video_data) + 8
        mdat_box = struct.pack('>I', mdat_size) + b'mdat' + dummy_video_data
        
        # Combine all boxes to create a valid MP4 structure
        mp4_data = ftyp_box + moov_box + mdat_box
        
        return mp4_data

def main():
    """Main benchmark execution function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="MAIF Benchmark Suite")
    parser.add_argument("--output-dir", default="benchmark_results",
                       help="Output directory for results")
    parser.add_argument("--quick", action="store_true",
                       help="Run quick benchmark (reduced test sizes)")
    
    args = parser.parse_args()
    
    # Initialize benchmark suite
    suite = MAIFBenchmarkSuite(args.output_dir)
    
    # Adjust test sizes for quick mode
    if args.quick:
        suite.text_sizes = [1024, 10240]  # Smaller sizes
        suite.embedding_counts = [100, 1000]
        suite.file_counts = [10, 100]
        print("Running in quick mode with reduced test sizes")
    
    # Run all benchmarks
    try:
        report = suite.run_all_benchmarks()
        
        # Print final summary
        print("\n" + "="*80)
        print("FINAL ASSESSMENT")
        print("="*80)
        
        assessment = report["overall_assessment"]
        print(f"Implementation Maturity: {assessment['implementation_maturity']}")
        print(f"Paper Claims Validated: {assessment['claims_met']}/{assessment['total_claims']} ({assessment['claims_percentage']:.1f}%)")
        print(f"Successful Benchmarks: {report['successful_benchmarks']}/{report['total_benchmarks']}")
        
        # Recommendations
        print("\nRECOMMENDATIONS:")
        if assessment['claims_percentage'] >= 80:
            print("✓ Implementation successfully validates most paper claims")
            print("✓ Ready for production use in appropriate domains")
        elif assessment['claims_percentage'] >= 60:
            print("⚠ Implementation validates majority of claims but needs improvement")
            print("⚠ Suitable for beta testing and development")
        else:
            print("✗ Implementation does not validate key paper claims")
            print("✗ Requires significant development before production use")
        
        return 0 if assessment['claims_percentage'] >= 60 else 1
        
    except Exception as e:
        print(f"Benchmark suite failed: {e}")
        return 1


if __name__ == "__main__":
    exit(main())